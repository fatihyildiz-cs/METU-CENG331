#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
    # corrupt all the unused registers to prevent assumptions
    irmovq $0x5710331, %rax
    irmovq $0x5710331, %rcx
    irmovq $0x5710331, %rbp
    irmovq $0x5710331, %r8
    irmovq $0x5710331, %r9
    irmovq $0x5710331, %r10
    irmovq $0x5710331, %r11
    irmovq $0x5710331, %r12
    irmovq $0x5710331, %r13
    irmovq $0x5710331, %r14
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Describe how and why you modified the baseline code.
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
	irmovq $0, %rax		# count = 0;
	andq %rdx,%rdx		# len <= 0?
	jle Done		# if len <= 0, go to Done
	iaddq $-8, %rdx
	jl thereAreLeftovers	# if 0 < length < 8, go to leftOvers
	# for 8 <= length, fall

Loop:
	mrmovq (%rdi), %r8	# r8 = first val
	mrmovq 8(%rdi), %r9	# r9 = second val
	mrmovq 16(%rdi), %r10	# r10 = third val
	mrmovq 24(%rdi), %r11	# r11 = third val

	andq %r8, %r8	# check if first val is positive
	jle cp1 				# if not pos, got to c1
	iaddq $1, %rax  # else, increment rax

cp1:
	rmmovq %r8, (%rsi)	# write the first val
	rmmovq %r9, 8(%rsi)	# write the second val
	rmmovq %r10, 16(%rsi)	# write the second val
	rmmovq %r11, 24(%rsi)	# write the second val

	andq %r9, %r9	# check if second val is positive
	jle cp2 # if not pos, go to cp2
	iaddq $1, %rax  # else, increment rax

cp2:
	andq %r10, %r10	# check if third val is positive
	jle cp3 # if not pos, go to cp3
	iaddq $1, %rax  # else, increment rax

cp3:
	andq %r11, %r11	# check if fourth val is positive
	jle cp4 # if not pos, go to cp3
	iaddq $1, %rax  # else, increment rax

cp4:
	mrmovq 32(%rdi), %r8	# r8 = fifth val
	mrmovq 40(%rdi), %r9	# r9 = sixth val
	mrmovq 48(%rdi), %r10	# r10 = seventh val
	mrmovq 56(%rdi), %r11	# r11 = eighth val

	andq %r8, %r8	# check if fifth val is positive
	jle cp5 				# if not pos, got to c1
	iaddq $1, %rax  # else, increment rax
	
cp5:
	rmmovq %r8, 32(%rsi)	# write the fifth val
	rmmovq %r9, 40(%rsi)	# write the sixth val
	rmmovq %r10, 48(%rsi)	# write the seventh val
	rmmovq %r11, 56(%rsi)	# write the eighth val

	andq %r9, %r9	# check if sixth val is positive
	jle cp6 # if not pos, go to cp2
	iaddq $1, %rax  # else, increment rax

cp6:
	andq %r10, %r10	# check if seventh val is positive
	jle cp7 # if not pos, go to cp2
	iaddq $1, %rax  # else, increment rax

cp7:
	andq %r11, %r11	# check if eighth val is positive
	jle inc # if not pos, go to cp2
	iaddq $1, %rax  # else, increment rax

inc:
	iaddq $64, %rdi		# src += 64
	iaddq $64, %rsi		# dst += 64
	iaddq	$-8, %rdx
	jge Loop			# if so, goto Loop:

thereAreLeftovers:
	iaddq $7, %rdx

r1:
	jl Done
	mrmovq (%rdi), %r10
	iaddq $8, %rdi		# src++
	rmmovq %r10, (%rsi)
	iaddq $8, %rsi		# dst++
	andq %r10, %r10
	jle r2
	iaddq $1, %rax
r2:
	iaddq  $-1, %rdx       # len--
	jl Done
	mrmovq (%rdi), %r10
	iaddq $8, %rdi		# src++
	rmmovq %r10, (%rsi)
	iaddq $8, %rsi		# dst++
	andq %r10, %r10
	jle r3
	iaddq $1, %rax
r3:
	iaddq  $-1, %rdx       # len--
	jl Done
	mrmovq (%rdi), %r10
	iaddq $8, %rdi		# src++
	rmmovq %r10, (%rsi)
	iaddq $8, %rsi		# dst++
	andq %r10, %r10
	jle r4
	iaddq $1, %rax
r4:
	iaddq  $-1, %rdx       # len--
	jl Done
	mrmovq (%rdi), %r10
	iaddq $8, %rdi		# src++
	rmmovq %r10, (%rsi)
	iaddq $8, %rsi		# dst++
	andq %r10, %r10
	jle r5
	iaddq $1, %rax
r5:
	iaddq  $-1, %rdx       # len--
	jl Done
	mrmovq (%rdi), %r10
	iaddq $8, %rdi		# src++
	rmmovq %r10, (%rsi)
	iaddq $8, %rsi		# dst++
	andq %r10, %r10
	jle r6
	iaddq $1, %rax
r6:
	iaddq  $-1, %rdx       # len--
	jl Done
	mrmovq (%rdi), %r10
	iaddq $8, %rdi		# src++
	rmmovq %r10, (%rsi)
	iaddq $8, %rsi		# dst++
	andq %r10, %r10
	jle r7
	iaddq $1, %rax
r7:
	iaddq  $-1, %rdx       # len--
	jl Done
	mrmovq (%rdi), %r10
	iaddq $8, %rdi		# src++
	rmmovq %r10, (%rsi)
	iaddq $8, %rsi		# dst++
	andq %r10, %r10
	jle Done
	iaddq $1, %rax


##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad 1
	.quad -2
	.quad -3
	.quad -4
	.quad -5
	.quad -6
	.quad -7
	.quad 8
	.quad -9
	.quad -10
	.quad -11
	.quad 12
	.quad -13
	.quad -14
	.quad 15
	.quad 16
	.quad -17
	.quad 18
	.quad 19
	.quad -20
	.quad -21
	.quad -22
	.quad 23
	.quad 24
	.quad 25
	.quad -26
	.quad -27
	.quad -28
	.quad -29
	.quad 30
	.quad -31
	.quad 32
	.quad 33
	.quad -34
	.quad -35
	.quad -36
	.quad 37
	.quad -38
	.quad 39
	.quad -40
	.quad -41
	.quad 42
	.quad -43
	.quad -44
	.quad 45
	.quad -46
	.quad -47
	.quad -48
	.quad -49
	.quad 50
	.quad 51
	.quad 52
	.quad 53
	.quad 54
	.quad 55
	.quad 56
	.quad 57
	.quad 58
	.quad 59
	.quad 60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
